{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ecec90",
   "metadata": {},
   "source": [
    "# Import the necessary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b993f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd2fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56259534",
   "metadata": {},
   "source": [
    "### code for extracting keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ab4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_keypoints(results):\n",
    "    face = np.array([[res.x, res.y, res.z] for res in np.array(results.multi_face_landmarks)[0].landmark]).flatten() if results.multi_face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([face])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034f41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mediapipe_detection(image, model):\n",
    "#    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "#    image.flags.writeable = False                  # Image is no longer writeable\n",
    "#    results = model.process(image)                 # Make prediction\n",
    "#    image.flags.writeable = True                   # Image is now writeable \n",
    "#    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "#    return image, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277e5ea",
   "metadata": {},
   "source": [
    "# Tester "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c349c34b",
   "metadata": {},
   "source": [
    "### set where we must looking for pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6efc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('E:\\Document\\FIDs_NEW') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['F0001', 'F0002', 'F0003'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b45791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(directory):\n",
    "    \"\"\"\n",
    "    This function will generate the file names in a directory tree by walking the tree either top-down or bottom-up. \n",
    "    For each directory in the tree rooted at directory top (including top itself), it yields a 3-tuple (dirpath, dirnames, filenames).\n",
    "    \"\"\"\n",
    "    file_paths = []  # List which will store all of the full filepaths.\n",
    "\n",
    "    # Walk the tree.\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                # Join the two strings in order to form the full filepath.\n",
    "                filepath = os.path.join(root, filename)\n",
    "                file_paths.append(filepath)  # Add it to the list.\n",
    "\n",
    "    return file_paths  # Self-explanatory.\n",
    "\n",
    "# Run the above function and store its results in a variable.   \n",
    "full_file_paths = get_filepaths(DATA_PATH)\n",
    "pic_number = len(full_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26f59eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Document\\\\FIDs_NEW\\\\F1000\\\\MID9\\\\P10578_face4.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_file_paths[1] # test for path pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f89cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_nameList = [string[-16:] for string in full_file_paths]\n",
    "image_nameList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8f89cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Document\\\\FIDs_NEW\\\\F0001\\\\MID1\\\\P00002_face3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_nameList = [string[:-4] for string in full_file_paths]\n",
    "image_nameList[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac5b28b6",
   "metadata": {},
   "source": [
    "## this for collecting the keypoints (looking a file in folder directory, then loop the code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73942622",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(pic_number):\n",
    "                  \n",
    "      # For static images:\n",
    "      IMAGE_FILES = [full_file_paths[i]]\n",
    "      drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "      with mp_face_mesh.FaceMesh(\n",
    "          static_image_mode=True,\n",
    "          max_num_faces=1,\n",
    "          refine_landmarks=True,\n",
    "          min_detection_confidence=0.5) as face_mesh:\n",
    "        for idx, file in enumerate(IMAGE_FILES):\n",
    "          image = cv2.imread(file)\n",
    "          # Convert the BGR image to RGB before processing. \n",
    "          results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "          # Print and draw face mesh landmarks on the image.\n",
    "          if not results.multi_face_landmarks:\n",
    "            continue\n",
    "          annotated_image = image.copy()\n",
    "          for face_landmarks in results.multi_face_landmarks:\n",
    "            #print('face_landmarks:', face_landmarks)\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_tesselation_style())\n",
    "            #mp_drawing.draw_landmarks(\n",
    "            #    image=annotated_image,\n",
    "            #    landmark_list=face_landmarks,\n",
    "            #    connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            #    landmark_drawing_spec=None,\n",
    "            #    connection_drawing_spec=mp_drawing_styles\n",
    "            #    .get_default_face_mesh_contours_style())\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_iris_connections_style())\n",
    "          # cv2.imwrite('videos' + str(idx) + '.png', annotated_image)  # for save image with keypoints\n",
    "          keypoints = extract_face_keypoints(results)  # for extract keypoints into array\n",
    "          npy_path = os.path.join(image_nameList[i])\n",
    "          np.save(npy_path, keypoints)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb24b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f2bd01b",
   "metadata": {},
   "source": [
    "## this code used if we will use a video (not yet added features to dettect files in directory so must do it manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72fca7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture('videos/ss_003_2.mp4')\n",
    "\n",
    "with mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    while True:\n",
    "        #cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Detect the face landmarks\n",
    "        results = face_mesh.process(image)\n",
    "        \n",
    "        # To improve performance\n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        # Convert back to the BGR color space\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                   .get_default_face_mesh_tesselation_style())\n",
    "#                mp_drawing.draw_landmarks(\n",
    "#                    image=image,\n",
    "#                    landmark_list=face_landmarks,\n",
    "#                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "#                    landmark_drawing_spec=None,\n",
    "#                    connection_drawing_spec=mp_drawing_styles\n",
    "#                    .get_default_face_mesh_contours_style())\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_iris_connections_style())\n",
    "                \n",
    "        # Display the image\n",
    "        cv2.imshow('MediaPipe FaceMesh', image)\n",
    "        \n",
    "        \n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "186fb703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49551976,  0.43732437, -0.06846248,  0.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tester code for collect array\n",
    "\n",
    "ress = np. array(results.multi_face_landmarks)\n",
    "resss = ress[0].landmark\n",
    "faces = np.array([[res.x, res.y, res.z, res.visibility] for res in resss])\n",
    "faces[1]\n",
    "#face = np.array([[res.x, res.y, res.z] for res in results.multi_face_landmarks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba84e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64376ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "export const MESH_ANNOTATIONS: {[key: string]: number[]} = {\n",
    "  silhouette: [\n",
    "    10,  338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,\n",
    "    397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,\n",
    "    172, 58,  132, 93,  234, 127, 162, 21,  54,  103, 67,  109\n",
    "  ],\n",
    "\n",
    "  lipsUpperOuter: [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291],\n",
    "  lipsLowerOuter: [146, 91, 181, 84, 17, 314, 405, 321, 375, 291],\n",
    "  lipsUpperInner: [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308],\n",
    "  lipsLowerInner: [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308],\n",
    "\n",
    "  rightEyeUpper0: [246, 161, 160, 159, 158, 157, 173],\n",
    "  rightEyeLower0: [33, 7, 163, 144, 145, 153, 154, 155, 133],\n",
    "  rightEyeUpper1: [247, 30, 29, 27, 28, 56, 190],\n",
    "  rightEyeLower1: [130, 25, 110, 24, 23, 22, 26, 112, 243],\n",
    "  rightEyeUpper2: [113, 225, 224, 223, 222, 221, 189],\n",
    "  rightEyeLower2: [226, 31, 228, 229, 230, 231, 232, 233, 244],\n",
    "  rightEyeLower3: [143, 111, 117, 118, 119, 120, 121, 128, 245],\n",
    "\n",
    "  rightEyebrowUpper: [156, 70, 63, 105, 66, 107, 55, 193],\n",
    "  rightEyebrowLower: [35, 124, 46, 53, 52, 65],\n",
    "\n",
    "  rightEyeIris: [473, 474, 475, 476, 477],\n",
    "\n",
    "  leftEyeUpper0: [466, 388, 387, 386, 385, 384, 398],\n",
    "  leftEyeLower0: [263, 249, 390, 373, 374, 380, 381, 382, 362],\n",
    "  leftEyeUpper1: [467, 260, 259, 257, 258, 286, 414],\n",
    "  leftEyeLower1: [359, 255, 339, 254, 253, 252, 256, 341, 463],\n",
    "  leftEyeUpper2: [342, 445, 444, 443, 442, 441, 413],\n",
    "  leftEyeLower2: [446, 261, 448, 449, 450, 451, 452, 453, 464],\n",
    "  leftEyeLower3: [372, 340, 346, 347, 348, 349, 350, 357, 465],\n",
    "\n",
    "  leftEyebrowUpper: [383, 300, 293, 334, 296, 336, 285, 417],\n",
    "  leftEyebrowLower: [265, 353, 276, 283, 282, 295],\n",
    "\n",
    "  leftEyeIris: [468, 469, 470, 471, 472],\n",
    "\n",
    "  midwayBetweenEyes: [168],\n",
    "\n",
    "  noseTip: [1],\n",
    "  noseBottom: [2],\n",
    "  noseRightCorner: [98],\n",
    "  noseLeftCorner: [327],\n",
    "\n",
    "  rightCheek: [205],\n",
    "  leftCheek: [425]\n",
    "link for picture https://github.com/google/mediapipe/blob/a908d668c730da128dfa8d9f6bd25d519d006692/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n",
    "\n",
    "478 * 3 = 1434 keypoints\n",
    "\n",
    "\n",
    "kinsip\n",
    "mother\n",
    "daughter\n",
    "son\n",
    "father\n",
    "\n",
    "1\t=\tanak dari\n",
    "2\t=\tsaudara dari\n",
    "3\t=\tcucu dari\n",
    "4\t=\torangtua dari\n",
    "5\t=\tistri/suami\n",
    "6\t=\tkakek/nenek dari\n",
    "\n",
    "MID\t1\t2\t3\t4\t5\t6\t\t\t\t\t\tGender\tName\n",
    "1\t  0\t5\t4\t4\t4\t1\t\t\t\t\t\tMale\t  Juan Esteban\n",
    "2\t  5\t0\t4\t4\t4\t0\t\t\t\t\t\tFemale\tkaren\n",
    "3\t  1\t1\t0\t2\t2\t3\t\t\t\t\t\tFemale\tPaloma\n",
    "4\t  1\t1\t2\t0\t2\t3\t\t\t\t\t\tMale\t  Dante\n",
    "5\t  1\t1\t2\t2\t0\t3\t\t\t\t\t\tFemale\tLuna\n",
    "6\t  4\t0\t6\t6\t6\t0\t\t\t\t\t\tFemale\tAlicia\n",
    "\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ec6c5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ferym\\dataset\\face points detector.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ferym/dataset/face%20points%20detector.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ferym/dataset/face%20points%20detector.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Detect the face landmarks\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ferym/dataset/face%20points%20detector.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m results \u001b[39m=\u001b[39m face_mesh\u001b[39m.\u001b[39;49mprocess(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ferym/dataset/face%20points%20detector.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# To improve performance\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ferym/dataset/face%20points%20detector.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mediapipe\\python\\solutions\\face_mesh.py:124\u001b[0m, in \u001b[0;36mFaceMesh.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[0;32m    110\u001b[0m   \u001b[39m\"\"\"Processes an RGB image and returns the face landmarks on each detected face.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39m    face landmarks on each detected face.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[0;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " #input:===================================================================================\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture('videos/ss_003_2.mp4')\n",
    "frame_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "with mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    for frame_num in range(frame_length):\n",
    "        #cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Detect the face landmarks\n",
    "        results = face_mesh.process(image)\n",
    "        \n",
    "        # To improve performance\n",
    "        image.flags.writeable = True\n",
    "\n",
    "        # Convert back to the BGR color space\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                   .get_default_face_mesh_tesselation_style())\n",
    "\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "        # Display the image\n",
    "        if frame_num == 0:\n",
    "            cv2.putText(image, 'STARTING COLLECTION', (120,200),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'Collecting frames num {} Video {}'.format(frame_num, 'test'), (70,50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            # Show to screen\n",
    "            cv2.imshow('MediaPipe FaceMesh', image)\n",
    "            cv2.waitKey(500)\n",
    "        else:\n",
    "            cv2.putText(image, 'Collecting frames num {} Video {} '.format(frame_num, 'test'), (15,12), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            # Show to screen\n",
    "            cv2.imshow('MediaPipe FaceMesh', image)\n",
    "                \n",
    "        keypoints = extract_face_keypoints(results)\n",
    "        npy_path = os.path.join(DATA_PATH, \"male\", str(frame_num))\n",
    "        np.save(npy_path, keypoints)\n",
    "\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f349d1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1434"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaderr = np.load('E:\\Document\\FIDs_NEW\\F0001\\MID1\\P00002_face3.npy') # to check our keypoints that we collects\n",
    "len(loaderr)   #lengt each array about 478 * 3 = 1434 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a45599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.48946536,  0.674936  , -0.06828458, ...,  0.70081866,\n",
       "        0.31247622, -0.04326841])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaderr\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e57284b21400dd436fa5076e6ab160f30379e8ab14ddc7b69576709516bce75d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
